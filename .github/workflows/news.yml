name: Crawl & Store News

on:  
  schedule:  
    - cron: "*/15 * * * *" # 每 15 分鐘  
  workflow_dispatch: # 允許手動觸發  
  push:
    paths:  
      - "scripts/**"  
      - ".github/workflows/news.yml"
      - "requirements.txt"  
      - "tmp/**"
      
permissions:  
  contents: write # 允許自動 commit

jobs:  
  run:  
    runs-on: ubuntu-latest  
    steps:  
      - name: Checkout  
        uses: actions/checkout@v4  
        with:
          persist-credentials: false # 我們用 PAT 或 actions/checkout+git push（二擇一）

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          
      - name: Install deps
        run: |
          python -m pip install -U pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      # 你自己的爬蟲邏輯：請把抓到的一批新聞輸出到 tmp/news_batch.json
      - name: Run your crawler (replace this step)
        run: |
          mkdir -p tmp
          python scripts/news.py

      - name: Store & dedupe
        run: |
          python scripts/store_news.py
          ls -l data || true
          git status

      # commit 變更（若有）
      - name: Configure git
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
      - name: Commit and push generated images
        run: |
          git add images/
          git commit -m "chore: add data on $(date -u '+%Y-%m-%d %H:%M:%S UTC')" || echo "No changes to commit"
          git remote set-url origin https://x-access-token:${GITHUB_TOKEN}@github.com/${{ github.repository }}.git
          git push origin HEAD:main
          
